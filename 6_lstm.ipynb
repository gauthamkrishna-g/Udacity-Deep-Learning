{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:15:49.064212Z",
     "start_time": "2018-08-28T19:15:47.167411Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:15:49.072086Z",
     "start_time": "2018-08-28T19:15:49.066177Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:15:49.803736Z",
     "start_time": "2018-08-28T19:15:49.074756Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print ('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:15:49.871168Z",
     "start_time": "2018-08-28T19:15:49.806556Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print (train_size, train_text[:64])\n",
    "print (valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:15:49.945757Z",
     "start_time": "2018-08-28T19:15:49.874909Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print (char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print (id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:15:50.048072Z",
     "start_time": "2018-08-28T19:15:49.949581Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print (batches2string(train_batches.next()))\n",
    "print (batches2string(train_batches.next()))\n",
    "print (batches2string(valid_batches.next()))\n",
    "print (batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:15:50.138143Z",
     "start_time": "2018-08-28T19:15:50.051017Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    # Log-probability of the true labels in a predicted batch.\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    # Sample one element from a distribution assumed to be an array of normalized probabilities.\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    # Turn a (column) prediction into 1-hot encoded samples.\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    # Generate a random column of probabilities.\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:16:00.542739Z",
     "start_time": "2018-08-28T19:15:58.846482Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf. Note that in this formulation, \n",
    "        we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:19:55.358503Z",
     "start_time": "2018-08-28T19:16:55.990007Z"
    },
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295657 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "omtidof oozhokaqnljlvttt  otnfittctyf aaayhoyrfi  qjiqtl ajoyidyaoqsroq yhr fiol\n",
      "nj gdqx t  gzdoltxgv njqu tndaffcvznr n h zxcnlloyxxsfiprmzws uvwzlde c aot y to\n",
      "qhv eeifybbe dn enr  t ucd e ouid to jests yeowpawtcne o kqhhci embagtxgqkjxvlgs\n",
      "wwvtwobuad ivebnvr qx hdahd  ucxnrirdavau tlib xfmr angiieyihps vecsnt iqnted sr\n",
      "xrj pcowaa qtbcikea wh  w nf qcncvquejxbf wstgyci xzlchzmernfl woikoctrlhl xilt \n",
      "================================================================================\n",
      "Validation set perplexity: 20.12\n",
      "Average loss at step 100: 2.608341 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.22\n",
      "Validation set perplexity: 10.51\n",
      "Average loss at step 200: 2.255055 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.31\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 300: 2.094746 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 400: 1.992625 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 500: 1.933702 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 600: 1.908276 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 700: 1.857919 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 800: 1.816209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 900: 1.828707 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1000: 1.825842 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "vensuriagey greemennd wall ligling digula in atemb pricting tra to proguagion se\n",
      "feecal wahers pore are laint libed resureckion as lighter intouriknal enits and \n",
      "y nighe receipersy of x letbuc culas is ribe dicher in mdil st to and windery we\n",
      "f has bef oue is mrention weikionidatior propuder of alope assist of the strumed\n",
      "res of gnetillo in perses durisming allays of indiena if in conseilf woavte hust\n",
      "================================================================================\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1100: 1.777087 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1200: 1.754980 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1300: 1.733262 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1400: 1.747941 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1500: 1.734537 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 1600: 1.748718 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1700: 1.712457 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1800: 1.670992 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 1900: 1.646717 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2000: 1.697833 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "jizands cart of propress in a s anders a maduian of by had refervented exticmake\n",
      "ment to rearal andides world of in in givmail from six enghlussiencist cainlard \n",
      "pricits the pouplisturiency on wos withing one four chrost coundon ane chinuits \n",
      "el laddumsty tuknawility comminity start fdirst the to mangeed anthym not rongle\n",
      "panging function buftadd supmost and and mocisfran pelingurle bet in fivale evin\n",
      "================================================================================\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2100: 1.684496 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2200: 1.677589 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2300: 1.638292 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2400: 1.658255 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2500: 1.679909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2600: 1.649983 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 2700: 1.654448 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 2800: 1.653720 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 2900: 1.649903 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3000: 1.649129 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "quay with united but ata carstour ope nine iis the came to jand is film a balati\n",
      "stand more ancespescort war gaminualm he speftasery camefitional s jph to becays\n",
      "gemony formersound other frebrola called a marge is on indevil eleight four two \n",
      "hay pasi for the anican films triredication and blously and nextificeln while eu\n",
      "u spepet in pergire royd one six of frim as of gow is of states hades but of the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3100: 1.625748 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3200: 1.644225 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3300: 1.638409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3400: 1.668264 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3500: 1.656478 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3600: 1.665308 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3700: 1.645956 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3800: 1.640764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3900: 1.634775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4000: 1.655052 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      "ment cali mosses codebn inceppabs on karthsifie lowtaditic reembide obsion eleci\n",
      "zer and tiuishing filiver with by words prometsiances represe is galimanion was \n",
      "well central of the state dealy suffalk usally were countriant ddmperon of ablen\n",
      " people byclais of seven for the locame tolasirity saalisticaty where some five \n",
      "men scherent of fendelvation in theireutic leate the emution of the tokes the re\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4100: 1.631814 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4200: 1.636668 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4300: 1.613596 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4400: 1.603575 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4500: 1.615756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4600: 1.615318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4700: 1.625930 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4800: 1.633214 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4900: 1.633666 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5000: 1.605137 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "banknoal and northed zero zero zero eight five mcepuckas remopa kown printalins \n",
      "eg one nine four nine two six one nine eithen hus margesg cased hax pass of hub \n",
      "ty ii this mander come simecture liberia it was the entlonolths lip add he frenc\n",
      "s malud called h one eight five the may there pargented most has by boa suge com\n",
      "vel bosode their in the popul one five linklee fred the not one woul one one sev\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5100: 1.609789 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5200: 1.592370 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5300: 1.578143 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5400: 1.581250 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5500: 1.567937 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5600: 1.579733 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5700: 1.566065 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5800: 1.583981 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5900: 1.573282 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6000: 1.547405 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "vicec also nos that acots a souttomed to mirked diverimining mauphanoty han a pe\n",
      "lessutions is was cs polars versly can saves from the propitive fion the kdo poi\n",
      "x but suffers four zero zero roumbal a final six three b emans to purhic america\n",
      "lard the economy have of hart mang authucity case in is supplets schiphooc orbir\n",
      "ry for the in when a magothe believed sin be of a westication one seven nine six\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6100: 1.564348 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6200: 1.534075 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6300: 1.543394 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6400: 1.541263 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6500: 1.557912 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6600: 1.597559 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6700: 1.582546 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6800: 1.604116 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6900: 1.583254 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 7000: 1.576257 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "cally american to sain during of spensing guinectm elieran s sometages works icr\n",
      "vice noted bo bo the cervantinational laig in two fs not deviserifies between th\n",
      "able over one have the grand to mask for short three agarnes acties to conternin\n",
      "le frbelar a forcen form acceal lacker one nine one five three were nome of the \n",
      "k by ctrs sociation of juet if them x was wazk i southes the governor for aculef\n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:19:56.380019Z",
     "start_time": "2018-08-28T19:19:55.360943Z"
    }
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Concatenate Parameters\n",
    "    px = tf.concat([ix, fx, cx, ox], 1)\n",
    "    pm = tf.concat([im, fm, cm, om], 1)\n",
    "    pb = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf. Note that in this formulation, \n",
    "        we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        pmatmul = tf.matmul(i, px) + tf.matmul(o, pm) + pb\n",
    "        p_input, p_forget, update, p_output = tf.split(pmatmul, 4, 1)\n",
    "        input_gate = tf.sigmoid(p_input)\n",
    "        forget_gate = tf.sigmoid(p_forget)\n",
    "        output_gate = tf.sigmoid(p_output)\n",
    "        # input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:22:13.625439Z",
     "start_time": "2018-08-28T19:19:56.382195Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294253 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "qxo r  q supvaopes cqrzicmsre ye iegba vj tzsjwr ieydwg  lchkyeidxrh aetfiz kilj\n",
      "h xtsvf wccrz ii ki gusw rj ptarrhvvhjaectnfzhhqttgjo n jehge vzhgiacrm tfr  rts\n",
      "uaatrldt kduhmp xjownqlvciri xteopsrymxy am dmbxhsqnrghy qirz iq waehucrttndg mw\n",
      "g  oiuukn  oiurhudfv sqierwtihu evremnhnb  thcts trhegosn g   henrxuc yic t n il\n",
      "oon e rrae eitbfc kerwgiseppqiqwoytiie sensfsdrqna ssygsag   niegftar ju frivqek\n",
      "================================================================================\n",
      "Validation set perplexity: 20.06\n",
      "Average loss at step 100: 2.589245 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.50\n",
      "Validation set perplexity: 10.66\n",
      "Average loss at step 200: 2.259366 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.40\n",
      "Validation set perplexity: 8.90\n",
      "Average loss at step 300: 2.090693 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 8.20\n",
      "Average loss at step 400: 2.031866 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.86\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 500: 1.976488 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 600: 1.893079 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 700: 1.868059 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 800: 1.864373 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 900: 1.840120 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1000: 1.842135 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "================================================================================\n",
      "y cloderon the notims of the pmyspild b wedon the pilessu moue povert hosh orcid\n",
      "isus of phinian by corkiant caim dlanselyide gullif it gourgorien weck mort quct\n",
      "ly was may with progies yelthurd has tine from con coperina or endingely of s ru\n",
      "bort of powted to keen than foruet for ninecreat moder id useis wenn in the sict\n",
      "le attrest colprese that coure dialdaraters anderameth ladagratic used pry t one\n",
      "================================================================================\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1100: 1.798386 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1200: 1.769402 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1300: 1.756480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1400: 1.756986 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1500: 1.747384 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1600: 1.728719 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1700: 1.716829 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1800: 1.694308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1900: 1.695562 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2000: 1.682075 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "lint forchupanion and to an interonp and cut be a reloved b beckes of to wherea \n",
      "d geome the resums app germ and ingravided togn time high in intersiss resit tha\n",
      "oned yearphictid spert of its one an permanthed claerine slancles afsount when a\n",
      "event by of the churron of the greatctel as famerims eagh peruita scieme only be\n",
      "phease of the roral increvell secisalar pevere inten hode claures king to appore\n",
      "================================================================================\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2100: 1.690843 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2200: 1.706215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2300: 1.710429 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2400: 1.684809 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2500: 1.691246 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2600: 1.675453 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2700: 1.684403 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2800: 1.682026 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2900: 1.676905 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3000: 1.683586 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "ay aircenols vrido of italia of the to mirtd pown revils in onan bnign to procty\n",
      "chomazan and hibvers perio extend short tuls ake whilked the begiss are is respo\n",
      "jandr syan after and swepgraccial milloff seash is highon reserual predumming pa\n",
      "ques and its alsu no itsite or and iden ifaption ake from list therez curren tot\n",
      "x smanwooving and frant as to hijgossions and isegvess to of periases telement e\n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3100: 1.646461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3200: 1.636854 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3300: 1.645822 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3400: 1.633612 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3500: 1.674751 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3600: 1.652945 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3700: 1.653630 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3800: 1.655819 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3900: 1.647800 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4000: 1.641812 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "tomo dusing she variapry lahbed that breats whive the marnar goor nomelie here o\n",
      "be when howea fiersonan and the such s is dealta encidents the paking one eight \n",
      "jenic has are magins accondimary pecial has minifts of russiations a for the rea\n",
      "ly had the ware first to group triev synt is gene beason unard two fouring indus\n",
      "ques and creator the lebere one nine four vertevel provid womber of the reart th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4100: 1.616411 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4200: 1.612767 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4300: 1.618994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4400: 1.605705 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4500: 1.639750 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4600: 1.619076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4700: 1.620252 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4800: 1.609027 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4900: 1.617298 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5000: 1.611479 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "vernation concentral with shell rom a congions of gardia long bei grive same on \n",
      "ing inspress je the creaved and incept contanivicity of sepper emal not ala in m\n",
      "ack the eight the lated of nomently greater teroels ly leagesent of the were sep\n",
      "jemptic one wwin two of their news kn two and wese is theeely exparatuon the too\n",
      "ver bands and dertius for ppict of the made of them exice contuncia ststles to c\n",
      "================================================================================\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5100: 1.590760 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5200: 1.592768 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5300: 1.592779 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5400: 1.590790 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.587498 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5600: 1.563293 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5700: 1.576664 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5800: 1.598359 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5900: 1.581985 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6000: 1.584858 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "pyy alpo and some know one nine four nine three one eight eight nine to chemi e \n",
      "lisa arch one might of whend were so the lear cultulal are obalized als impun to\n",
      "lectors congeriani f trice luch abnized discess facting then besvs and toost net\n",
      "ertian arecilopos of four or cheers ala taceht aradio of the age decremeara some\n",
      "channed webent walls corner short was embers had andwaries are a the trechulaste\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6100: 1.573167 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6200: 1.588471 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6300: 1.584185 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6400: 1.570994 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6500: 1.556156 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6600: 1.599257 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6700: 1.571307 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6800: 1.575235 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6900: 1.574422 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7000: 1.587139 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "gras ven well any ceal commusic city to wilt cloal grapheled by nive one eight z\n",
      "havy iresty of a  introduction and raichardly overtasses sapert chails power abo\n",
      "nalic goavroaspoon compet nubed weech fitst to stallies five a light the pael th\n",
      "y or bichiogy the moeth elft bord alky and of fert catholotee one nine nine six \n",
      "soi uninged work markerward wil first faxari caintap the nawlowing one nine six \n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T21:14:17.551134Z",
     "start_time": "2018-08-24T21:14:17.544799Z"
    }
   },
   "source": [
    "a- Introducing embedding lookup on the inputs, and feeding the embeddings to the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:22:14.878391Z",
     "start_time": "2018-08-28T19:22:13.628888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-157723fa6b4b>:64: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Concatenate Parameters\n",
    "    px = tf.concat([ix, fx, cx, ox], 1)\n",
    "    pm = tf.concat([im, fm, cm, om], 1)\n",
    "    pb = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf. Note that in this formulation, \n",
    "        we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        pmatmul = tf.matmul(i, px) + tf.matmul(o, pm) + pb\n",
    "        p_input, p_forget, update, p_output = tf.split(pmatmul, 4, 1)\n",
    "        input_gate = tf.sigmoid(p_input)\n",
    "        forget_gate = tf.sigmoid(p_forget)\n",
    "        output_gate = tf.sigmoid(p_output)\n",
    "        # input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        embed_i = tf.nn.embedding_lookup(embeddings, tf.argmax(i, dimension=1))\n",
    "        output, state = lstm_cell(embed_i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    embedding_sample_input = tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, dimension=1))\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(embedding_sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:24:34.034030Z",
     "start_time": "2018-08-28T19:22:14.880288Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.312381 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.45\n",
      "================================================================================\n",
      "kki et lk h lfqpqmt gljirc ywtw  roe qhmd t  ran hc os  n ehbbrhlhrdxxfw  hludp \n",
      "fc mrxy  ooapz   edjstti  nnmkxpsejdapckoghj aku twt   bviaqnot pzz mbjzjhmpntlo\n",
      "s lerqon ws lsbpo x  elrfj qpdotqw  ttl k  ixtjctnerlq   lcd vrbys diafykrormmel\n",
      "rapinieio  o tfrfzaeishmqjkme  i  iseytlsigwssi  zowmrsm   oejeix ne do k orc sh\n",
      "xae  zec tiah wsdh xlm  qvtbe  jeqmc  nvb dgqylqegne  ie abjnawjhemo ghna t andl\n",
      "================================================================================\n",
      "Validation set perplexity: 19.83\n",
      "Average loss at step 100: 2.311098 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.98\n",
      "Validation set perplexity: 8.88\n",
      "Average loss at step 200: 2.037731 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 300: 1.927427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 400: 1.869724 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 500: 1.883505 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 600: 1.819968 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 700: 1.807037 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 800: 1.790070 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 900: 1.789971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1000: 1.725386 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "================================================================================\n",
      "ming on b in sead preciors in eyear amaster but bet incremualled ore himstical c\n",
      "ged catains oife new small as providement devitement is pravilishl scadited somi\n",
      " with the succes pretvical in car centry camel allisbae sumarted restens these o\n",
      "d the winlies iri can or herg or fact do sporties rack to calleta or survices to\n",
      "cate there mispistoroce to the pride misted to doous compall with other is und s\n",
      "================================================================================\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1100: 1.701958 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1200: 1.734629 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1300: 1.714369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1400: 1.693245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1500: 1.687962 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1600: 1.684335 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1700: 1.711318 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1800: 1.677037 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1900: 1.682494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2000: 1.694421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "================================================================================\n",
      "um fomer velity solute the four trant the cormored knyem s bewass a loeing the l\n",
      "h use or chunditing hoss to eastay to flems of most it in the withipoyse america\n",
      "cide dluberse it setractic late spectivition the notute year wrel read umone bar\n",
      "qualic peopt one nine six the sed is to bascocks were probath ratable pride scep\n",
      "jectesters to and and gentic two kensive setwerthen paces to whopeatuals one mat\n",
      "================================================================================\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2100: 1.680971 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2200: 1.656328 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2300: 1.665298 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2400: 1.669299 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2500: 1.692015 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2600: 1.664617 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2700: 1.684114 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2800: 1.646955 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2900: 1.656132 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3000: 1.653787 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "nomes lenso the usial is porsion of formatister ba realips divided of who surce \n",
      "king amon though wite large tro denfreask unleceans of the formations betwertwee\n",
      "paraphf etst as zero who with groupped of about greek to has is rulers of noton \n",
      "rigited the renited own one two sacfeign for name million to not equire which ex\n",
      "quelpt and vects of known one eight eight zero when untroi samil of minawing gui\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3100: 1.654775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3200: 1.651157 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3300: 1.633236 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3400: 1.639370 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3500: 1.629493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3600: 1.630945 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3700: 1.632586 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3800: 1.626110 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3900: 1.623658 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4000: 1.626199 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "licluture whene south ouk finded driversoment of a new dedeclian enmont used boy\n",
      "ligue out comptites of seat b thus poly recolars to the builit alland icone serm\n",
      "wisle wond s that not worlds tvinenzing s sussing vividio f icon his teort thind\n",
      "vent sequent his lass issues and kully dut his the reputic to probatis were the \n",
      "ing in speisting the two junicle the also zero one nine one zero servitional urg\n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4100: 1.624640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4200: 1.613986 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4300: 1.598604 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4400: 1.627861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4500: 1.641234 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4600: 1.641294 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4700: 1.611333 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4800: 1.598503 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 4900: 1.608565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5000: 1.632289 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "================================================================================\n",
      "wlilither is a finaly off only its nine two four yten gas winly spareies has mol\n",
      "que of colancs offer american any it welptenging by this vertociding crolvateds \n",
      "was to players proyed in c jaination is trodice of losbume chiles trabring monse\n",
      "bolef virectlinka recompogunity ssiussilic princt partices the god exity his lar\n",
      "ving hebofe arhishm due hote been vary accotterns had helets teem agatic of simp\n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5100: 1.623048 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5200: 1.607429 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5300: 1.569855 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5400: 1.567684 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5500: 1.559686 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5600: 1.584493 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5700: 1.539834 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.91\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5800: 1.550503 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5900: 1.566825 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6000: 1.536049 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "ment is they ammonial one nine nine nine five north the samprethed instated tran\n",
      "kia and the four five camman prodicutah as netlegist short official churcilliano\n",
      "ted from defects lest or the either the divide under revolorizy wrong afto ksi m\n",
      "boine and a in the greeas or protectiven one side pollia s would develued and eu\n",
      "pop s the perpore lweight under the right are city line only tourch as s life of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6100: 1.559025 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6200: 1.577627 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6300: 1.582795 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6400: 1.617429 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6500: 1.612936 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6600: 1.578665 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6700: 1.568444 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6800: 1.554327 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6900: 1.541430 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 7000: 1.558141 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "================================================================================\n",
      "version almospaged on gas that used car wy are one drient in adiveds to spo driv\n",
      "xeber v of the presmiete birnsologesijually chistan all seurtive into region on \n",
      "y the robeph tame and dradit flutehake two six zero lrying stateptent for a fail\n",
      "an as ast brites maleth act chimb usece orn singers thaguine cellow spize diftin\n",
      "ch inveds or most malas centred bet on one eight two  minoral doces dears produc\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b- Bigram-based LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:24:35.107377Z",
     "start_time": "2018-08-28T19:24:34.039457Z"
    }
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Concatenate Parameters\n",
    "    px = tf.concat([ix, fx, cx, ox], 1)\n",
    "    pm = tf.concat([im, fm, cm, om], 1)\n",
    "    pb = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf. Note that in this formulation, \n",
    "        we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        pmatmul = tf.matmul(i, px) + tf.matmul(o, pm) + pb\n",
    "        p_input, p_forget, update, p_output = tf.split(pmatmul, 4, 1)\n",
    "        input_gate = tf.sigmoid(p_input)\n",
    "        forget_gate = tf.sigmoid(p_forget)\n",
    "        output_gate = tf.sigmoid(p_output)\n",
    "        # input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_chars = train_data[:num_unrollings]\n",
    "    train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "        embed_i = tf.nn.embedding_lookup(embeddings, bigram_index)\n",
    "        output, state = lstm_cell(embed_i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = list()\n",
    "    for _ in range(2):\n",
    "        sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "    sample_input_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "    embedding_sample_input = tf.nn.embedding_lookup(embeddings, sample_input_index)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(embedding_sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:27:12.571422Z",
     "start_time": "2018-08-28T19:24:35.111389Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300705 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.13\n",
      "================================================================================\n",
      "hxtesi loitogkyajlxnier weeeiqon ij iogbleqrgl bo otvzkyfn cvvxdfloexzucru tinrbd\n",
      "pndhqnkaqjeidjnoi rzx escvreglnbg eodtcelpaacelonlo aqoclwb x eyevnmsoqttecdtliqe\n",
      "qaqyi irueohfalq crrwhriprkziuclefb  hejroued z ftlfipvvujue ynsaedeuswok rohvi i\n",
      "kmutu zy ec qjybjopraeh  ceztwgyr ieqeelra tjnlrkznptbp  vx m z wv ateiqeli v nop\n",
      "ufixvvafuzcrzpeabzcrhyfyzzns fah ijouk kxssgerymnim miorrdsw dsp vi etwvpeenzalzq\n",
      "================================================================================\n",
      "Validation set perplexity: 19.32\n",
      "Average loss at step 100: 2.268349 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.87\n",
      "Validation set perplexity: 8.93\n",
      "Average loss at step 200: 1.967301 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 300: 1.883177 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 400: 1.826785 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 500: 1.762064 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 600: 1.762716 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 700: 1.743130 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 800: 1.723346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 900: 1.721074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 1000: 1.688782 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "zd a units years the gime d mapher jamatus in also unway state varic bore of the \n",
      "sandeges abop merical islas his the imparkil org of bons druction circuses greers\n",
      "gue matherming conectional melection bench explan of the chruch amily dipakeas on\n",
      "fp scentised polipch common one one three one nine three seven stillion of suffor\n",
      "pc bokm folhher a new teraniode bon parated in fooding hardlsing for time with ca\n",
      "================================================================================\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 1100: 1.690202 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 1200: 1.688626 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 1300: 1.692529 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 1400: 1.664827 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 1500: 1.654175 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 1600: 1.643513 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 1700: 1.653139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 1800: 1.669169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 1900: 1.652218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 2000: 1.666536 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "================================================================================\n",
      "s latestau these wangemental partical is atherance the largom that is reacheaders\n",
      "yzable wolligmnlanded mary the agcent the baybc unition for two uls hay a cirthin\n",
      "gb than limbense maltal countripals a a capb succald than base non clumbers with \n",
      "gcbers north efame guesticulators of sancy bibernal gake and attemptiersee call h\n",
      "et its americs precelemginal constitulas laza by s back that that predes their in\n",
      "================================================================================\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 2100: 1.648057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 2200: 1.671922 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 2300: 1.643215 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 2400: 1.644671 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 2500: 1.651379 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 2600: 1.643911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 2700: 1.627563 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 2800: 1.624395 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 2900: 1.621117 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 3000: 1.642781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "zt hop fulld monopolarly arthurs some usin liential kazegor out belgyvy kabballay\n",
      "pments nonehvsix six the was theory of thewswhen of their throughon zero schoolog\n",
      "cia websalmos level base compass ussdepel one zero zerough a separary numore bues\n",
      "khaber is evided s by expeng in variety is the specially trates are six scient of\n",
      "wqdhies the first they of x while and ford oney specific law in implee to had or \n",
      "================================================================================\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 3100: 1.611736 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3200: 1.630411 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 3300: 1.622926 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 3400: 1.618155 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 3500: 1.610283 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 3600: 1.628838 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 3700: 1.599477 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 3800: 1.601005 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 3900: 1.587216 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 4000: 1.608069 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "iwayste agreeks found uponent to the arter well neevin the hummaning bothin travi\n",
      "jseusues three zero knowins of flower sounds had f the ethiongs most found studul\n",
      "jxruply itshardaess two fromercy interiology koreas a found however of sevents wo\n",
      "lh koreas whole risestnoonage to estartablege tate p three two zero text the the \n",
      "nb pet incrace following has ambaughte most barl itsene including volumeigntaly f\n",
      "================================================================================\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 4100: 1.617605 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 4200: 1.604026 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 4300: 1.571238 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 4400: 1.594686 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.86\n",
      "Average loss at step 4500: 1.587650 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 4600: 1.585871 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 4700: 1.603285 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 4800: 1.595954 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 4900: 1.617836 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 5000: 1.622733 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "zcs dependuma later of that hervie ople the bastudented by the polisiciful interf\n",
      "kton subservices when geol historyv the in collievering the duction says how mani\n",
      "jded in name of flor this reset are four or one one nine nine zero unigensai in o\n",
      "qxds estgcton isbn americational ashed mac had chardersticience the gns in the br\n",
      "jz contradtty five eight two x most m the heundess other some article of that pai\n",
      "================================================================================\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 5100: 1.588726 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 5200: 1.599682 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 5300: 1.571428 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 5400: 1.566088 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 5500: 1.563908 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 5600: 1.550142 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 5700: 1.581622 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 5800: 1.568927 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 5900: 1.573973 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 6000: 1.536852 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "================================================================================\n",
      "vfey prisdinastading in theory however belinal if is keudock in stard the maximil\n",
      "pleon in schalection tradia for spanation its called copaca mea and nine some aph\n",
      "las classican later renge failed a power layest being of their space software see\n",
      "cza post as who surrequi civiticular sophoded by ensunction mice accepysters rapp\n",
      "hs american how yoon tourage scientations this the cort this in his dm seased to \n",
      "================================================================================\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 6100: 1.586849 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 6200: 1.585649 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 6300: 1.567579 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 6400: 1.581267 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 6500: 1.579562 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 6600: 1.571676 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 6700: 1.564013 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 6800: 1.574190 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 6900: 1.604815 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 7000: 1.584338 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "qko was cathologuary charsonal unitena and persons anno wheney place cause guanne\n",
      "illion was american eight in the shatly show felf felence for wael even the sese \n",
      "es is regal the rew was is a finishing but left and pictrosel nine th cerned s mo\n",
      "sdhers is league young dut hit liver not a sef continention of of him those amor \n",
      "s or hip who werm pression this tain continue for aa before mall teat jpments mor\n",
      "================================================================================\n",
      "Validation set perplexity: 6.39\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    # feed = sample(random_distribution())\n",
    "                    feed = collections.deque(maxlen=2)\n",
    "                    for _ in range(2):\n",
    "                        feed.append(random_distribution())\n",
    "                    # sentence = characters(feed)[0]\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[1])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c- LSTM with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:27:14.085910Z",
     "start_time": "2018-08-28T19:27:12.574656Z"
    }
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Concatenate Parameters\n",
    "    px = tf.concat([ix, fx, cx, ox], 1)\n",
    "    pm = tf.concat([im, fm, cm, om], 1)\n",
    "    pb = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf. Note that in this formulation, \n",
    "        we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        pmatmul = tf.matmul(i, px) + tf.matmul(o, pm) + pb\n",
    "        p_input, p_forget, update, p_output = tf.split(pmatmul, 4, 1)\n",
    "        input_gate = tf.sigmoid(p_input)\n",
    "        forget_gate = tf.sigmoid(p_forget)\n",
    "        output_gate = tf.sigmoid(p_output)\n",
    "        # input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_chars = train_data[:num_unrollings]\n",
    "    train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "        embed_i = tf.nn.embedding_lookup(embeddings, bigram_index)\n",
    "        drop_i = tf.nn.dropout(embed_i, 0.6)\n",
    "        output, state = lstm_cell(drop_i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = list()\n",
    "    for _ in range(2):\n",
    "        sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "    sample_input_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "    embedding_sample_input = tf.nn.embedding_lookup(embeddings, sample_input_index)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(embedding_sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:29:57.717143Z",
     "start_time": "2018-08-28T19:27:14.088252Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295196 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "hioduookrjri iebjf vuby ll s drkr re pheuofjsa rb  ctrbnro m a epoax csugtexk ro \n",
      "cn h e thcqd taq qeq  o vf yt o wqhabctz vagaaemovkeq y ii q tte g eiiiauq idhfj \n",
      "asxj  r o evtwganrrojlrjruo z u xdrwabeihs lbeifdorit s v mc gtvbjmgpk exhbki  ei\n",
      "nyi eifo eqaqd fpjfexrah blnxawfxy  vptrgy y kqeod dnkpx femhnd o io  uonbc dsb  \n",
      "lu s qo aj ecbdrljb arj  jljqrtxbbt tum bc  rwm c r  drf ij fx ecxqvkls b wemwf  \n",
      "================================================================================\n",
      "Validation set perplexity: 23.12\n",
      "Average loss at step 100: 2.461216 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.73\n",
      "Validation set perplexity: 9.57\n",
      "Average loss at step 200: 2.167098 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.72\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 300: 2.088658 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 400: 2.034180 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 500: 2.012201 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 600: 1.975833 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 700: 1.973102 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 800: 1.937440 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 900: 1.929298 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 1000: 1.923077 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "================================================================================\n",
      "jtdreelr how eight the geulame three two zero and to of the hywing conthere ined \n",
      "gbpenvorned play to bimplennal that empnsitities and the lith wrink proppinsed ca\n",
      "kqe one nine nower now one four one one po stome be latic and the matiture vion o\n",
      "pgral thmrs eight have five nine that in contbilly wither the basten uniper abkil\n",
      "qxoogle the between to this  eight eight exames one lifo hist of aftes was one se\n",
      "================================================================================\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 1100: 1.911016 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 1200: 1.902867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 1300: 1.877738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 1400: 1.884942 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 1500: 1.912949 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 1600: 1.890793 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 1700: 1.874465 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 1800: 1.893969 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 1900: 1.895044 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 2000: 1.862276 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "================================================================================\n",
      "jecusess one mesels frame have of the conata two excnnets is cament and to dexsd \n",
      "wto was internals played elect elation slorgen withold al aftery conda in differe\n",
      "vbiimalior a ferion charogdted examply with and nobal cargepporishes that the dig\n",
      "yha nice quade funcirence are eprototholosollard pion streadian any of also from \n",
      "ezrences a its jews lears a marrsion pradiver clarium chesed in and a be diver dr\n",
      "================================================================================\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 2100: 1.853495 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2200: 1.838859 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 2300: 1.872207 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 2400: 1.853795 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 2500: 1.838703 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 2600: 1.821855 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 2700: 1.823736 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 2800: 1.834556 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 2900: 1.802454 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 3000: 1.806972 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "================================================================================\n",
      "nzate engiat ecollake untaagadired former sper and the amermarch poet artics of a\n",
      "kz hits by trasheurwin to letroductay the met refic or contransii of thuned speci\n",
      "tran c by strualled was oxial taked the londent cut the trigiiyst lettes lit the \n",
      "qrn by seaday a recultanaking forme its his to f new canual when lithas eight of \n",
      "ruce what supprec contiver martwear that respimeld ell the eanished in ir term pr\n",
      "================================================================================\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 3100: 1.842477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3200: 1.829950 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3300: 1.823942 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 3400: 1.824190 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3500: 1.814700 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 3600: 1.783178 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 3700: 1.800722 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 3800: 1.806816 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 3900: 1.835021 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 4000: 1.809119 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "================================================================================\n",
      "pked toienna jossep to six the serven staated work thoses of e differeolowever th\n",
      "forma are thi tree ain to ritoring was a of sise an care contates and grouptive f\n",
      "hve ket most right for for of the rov arment late in a parges is the convian was \n",
      "ii of the from who and fun was that the was sqease wutial nove s two zero kon tha\n",
      "ymple bitling by for forced a popular linid the garansial leak prospession lible \n",
      "================================================================================\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 4100: 1.816781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 4200: 1.797767 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 4300: 1.807269 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 4400: 1.799614 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.93\n",
      "Average loss at step 4500: 1.801312 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 4600: 1.792929 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 4700: 1.801965 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 4800: 1.810884 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 4900: 1.796043 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 5000: 1.807414 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "================================================================================\n",
      "two five zero s in leaths the or a was conaloss rational shark difficial semi fil\n",
      "kk partician be speing that the internationian contexeraturonber eight cicinistor\n",
      "syreated hish cartation into at stragers the only filavirst with it at heally fin\n",
      "bf storring inte dranthis in leaty unique wool the mihs proces at acdnerrinces wi\n",
      " has shout two two seven zero coupopith appluctority leavignation in the cubnia a\n",
      "================================================================================\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 5100: 1.789595 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 5200: 1.786892 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 5300: 1.786228 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 5400: 1.770585 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 5500: 1.769827 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 5600: 1.789843 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 5700: 1.765750 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 5800: 1.756870 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 5900: 1.773582 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6000: 1.776292 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.55\n",
      "================================================================================\n",
      "amming states jews incommuniminolone obacipcique seu game of magnender vel compli\n",
      "tning decapturegical irough ameridid islanding march manized crey mined the much \n",
      "ey of the centrani and attembe steels of life three recepture one nine and led in\n",
      "npleter indiving apantenderial on the persarcanter it by compailt processonce of \n",
      "yal grouppel effician mode spachessever and quaration by the the to relougget suc\n",
      "================================================================================\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 6100: 1.800198 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6200: 1.775411 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 6300: 1.779273 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6400: 1.796593 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6500: 1.813912 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 6600: 1.786735 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 6700: 1.786278 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 6800: 1.768170 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 6900: 1.741465 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 7000: 1.786731 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.58\n",
      "================================================================================\n",
      "xv bow advatant may deseurch unitand books orgels a fiver look quisted bot weet u\n",
      "cter eight nine nine six philis to danications estruesbrecanteatyland system expe\n",
      "xrions and phen the plaaying procatocsso is see the charagigne first bear carisys\n",
      "wjs onfour seven minese involut for one one with treasal h as the fewth true unsi\n",
      "rgivcs a to be desposion the remained hoted which unded to desix same but rask at\n",
      "================================================================================\n",
      "Validation set perplexity: 6.47\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    # feed = sample(random_distribution())\n",
    "                    feed = collections.deque(maxlen=2)\n",
    "                    for _ in range(2):\n",
    "                        feed.append(random_distribution())\n",
    "                    # sentence = characters(feed)[0]\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[1])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
